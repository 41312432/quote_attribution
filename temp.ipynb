{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TransfoXLTokenizer, TransfoXLModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea914d88384c92aefd59d2e2b17a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2bbf44d2ba40cb99f7813b4da0c024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at transfo-xl-wt103 were not used when initializing TransfoXLModel: ['crit.out_layers.1.weight', 'crit.out_layers.0.bias', 'crit.out_projs.2', 'crit.out_layers.1.bias', 'crit.cluster_bias', 'crit.out_layers.3.bias', 'crit.out_layers.3.weight', 'crit.out_projs.3', 'crit.out_layers.2.weight', 'crit.out_projs.0', 'crit.cluster_weight', 'crit.out_layers.2.bias', 'crit.out_projs.1', 'crit.out_layers.0.weight']\n",
      "- This IS expected if you are initializing TransfoXLModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TransfoXLModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "modelx = TransfoXLModel.from_pretrained(\"transfo-xl-wt103\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizerx = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103', lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertEmbeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (token_type_embeddings): Embedding(2, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "TransfoXLConfig {\n",
      "  \"_name_or_path\": \"transfo-xl-wt103\",\n",
      "  \"adaptive\": true,\n",
      "  \"architectures\": [\n",
      "    \"TransfoXLLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": 0,\n",
      "  \"clamp_len\": 1000,\n",
      "  \"cutoffs\": [\n",
      "    20000,\n",
      "    40000,\n",
      "    200000\n",
      "  ],\n",
      "  \"d_embed\": 1024,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 4096,\n",
      "  \"d_model\": 1024,\n",
      "  \"div_val\": 4,\n",
      "  \"dropatt\": 0.0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"ext_len\": 0,\n",
      "  \"init\": \"normal\",\n",
      "  \"init_range\": 0.01,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"mem_len\": 1600,\n",
      "  \"model_type\": \"transfo-xl\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 18,\n",
      "  \"pre_lnorm\": false,\n",
      "  \"proj_init_std\": 0.01,\n",
      "  \"same_length\": true,\n",
      "  \"sample_softmax\": -1,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"tgt_len\": 128,\n",
      "  \"tie_projs\": [\n",
      "    false,\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"tie_weight\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"untie_r\": true,\n",
      "  \"vocab_size\": 267735\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings)\n",
    "print(model.config)\n",
    "print(modelx.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Mr. Collins had a compliment, and an allusion to throw in here, which were kindly smiled on by the mother and daughter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', '.', 'collins', 'had', 'a', 'compliment', ',', 'and', 'an', 'all', '##usion', 'to', 'throw', 'in', 'here', ',', 'which', 'were', 'kindly', 'smiled', 'on', 'by', 'the', 'mother', 'and', 'daughter', '.']\n",
      "[101, 2720, 1012, 6868, 2018, 1037, 19394, 1010, 1998, 2019, 2035, 14499, 2000, 5466, 1999, 2182, 1010, 2029, 2020, 19045, 3281, 2006, 2011, 1996, 2388, 1998, 2684, 1012, 102]\n",
      "['mr.', 'collins', 'had', 'a', 'compliment', ',', 'and', 'an', 'allusion', 'to', 'throw', 'in', 'here', ',', 'which', 'were', 'kindly', 'smiled', 'on', 'by', 'the', 'mother', 'and', 'daughter', '.']\n",
      "[24, 24, 32, 8, 23881, 2, 5, 31, 17357, 6, 5189, 7, 1744, 2, 34, 28, 29651, 47009, 15, 20, 1, 669, 5, 943, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(test))\n",
    "print(tokenizer.encode(test))\n",
    "print(tokenizerx.tokenize(test))\n",
    "print(tokenizerx.encode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "267735\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizerx.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pnp_name_list.txt\", 'r', encoding='utf-8') as fin:\n",
    "        name_lines = fin.readlines()\n",
    "\n",
    "alias_to_id = {}\n",
    "for i, line in enumerate(name_lines):\n",
    "    for alias in line.strip().split(';'):\n",
    "        alias_to_id[alias.lower()] = i\n",
    "\n",
    "for alias in alias_to_id:\n",
    "    tokenizer.add_tokens(alias.lower())\n",
    "    tokenizerx.add_tokens(alias.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "267735\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizerx.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr. collins', 'had', 'a', 'compliment', ',', 'and', 'an', 'all', '##usion', 'to', 'throw', 'in', 'here', ',', 'which', 'were', 'kindly', 'smiled', 'on', 'by', 'the', 'mother', 'and', 'daughter', '.']\n",
      "[101, 30572, 2018, 1037, 19394, 1010, 1998, 2019, 2035, 14499, 2000, 5466, 1999, 2182, 1010, 2029, 2020, 19045, 3281, 2006, 2011, 1996, 2388, 1998, 2684, 1012, 102]\n",
      "[CLS] mr. collins had a compliment, and an allusion to throw in here, which were kindly smiled on by the mother and daughter. [SEP]\n",
      "['mr. collins', 'had', 'a', 'compliment', ',', 'and', 'an', 'allusion', 'to', 'throw', 'in', 'here', ',', 'which', 'were', 'kindly', 'smiled', 'on', 'by', 'the', 'mother', 'and', 'daughter', '.']\n",
      "[267795, 32, 8, 23881, 2, 5, 31, 17357, 6, 5189, 7, 1744, 2, 34, 28, 29651, 47009, 15, 20, 1, 669, 5, 943, 3]\n",
      "mr. collins had a compliment, and an allusion to throw in here, which were kindly smiled on by the mother and daughter.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(test))\n",
    "print(tokenizer.encode(test))\n",
    "print(tokenizer.decode(tokenizer.encode(test)))\n",
    "print(tokenizerx.tokenize(test))\n",
    "print(tokenizerx.encode(test))\n",
    "print(tokenizerx.decode(tokenizerx.encode(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bert import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_examples_to_features(examples=[test], tokenizer=tokenizerx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wansik/QA/quote_attribution/temp.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Biknow-gold/home/wansik/QA/quote_attribution/temp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m bert_output \u001b[39m=\u001b[39m modelx(torch\u001b[39m.\u001b[39;49mtensor([features[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49minput_ids], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)), token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, attention_mask\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mtensor([features[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49minput_mask], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "bert_output = modelx(torch.tensor([features[0].input_ids], dtype=torch.long).to(torch.device(\"cpu\")), token_type_ids=None, attention_mask=torch.tensor([features[0].input_mask], dtype=torch.long).to(torch.device(\"cpu\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(alias_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveEmbedding(\n",
       "  (emb_layers): ModuleList(\n",
       "    (0): Embedding(20000, 1024)\n",
       "    (1): Embedding(20000, 256)\n",
       "    (2): Embedding(160000, 64)\n",
       "    (3): Embedding(67837, 16)\n",
       "  )\n",
       "  (emb_projs): ParameterList(\n",
       "      (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "      (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "      (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "      (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx.resize_token_embeddings(tokenizerx.vocab_size+102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output = modelx(torch.tensor([features[0].input_ids], dtype=torch.long).to(torch.device(\"cpu\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55318ce83133ef078d3570415b5105a74962c2b6e94ef4711f08cb7d1a9ada90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
